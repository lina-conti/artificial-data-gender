context_model : 'GPT' #one of 'RNN', 'LSTM', 'GPT'
model_input_size:  [768]
model_output_size: [768]
num_layers: [16]
max_vocab_size: 50000 #Gulordava setup
nheads : [16]  #for GPT only
ffn_hidden : [2048]   #for GPT only
tie_weights : True
dropout: [0.0]
epochs: [100]
batch_size: [64] 
bptt_chunk : [150]     #size of context for truncated BPTT
learning_rate: [0.02]
warmup_epochs: [1]       #number of epochs for warmup
warmup_batch_size : [8]  #size of batches during warmup
restart_cycles: [4]      #number or warmup restarts for GPT only
positional : True        #use positional embeddings or ignore them
