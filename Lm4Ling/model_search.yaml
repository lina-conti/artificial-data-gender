context_model : 'RNN' #one of 'RNN', 'LSTM', 'GPT'
model_input_size:  [256,512,768]
model_output_size: [256,512,768]
num_layers: [1]
max_vocab_size: 50000 #Gulordava setup
tie_weights : True
nheads : [1]            #for GPT only
ffn_hidden : [2048]     #for GPT only
dropout: [0.5,0.75]
epochs:  [4]
batch_size: [64]
bptt_chunk : [150]      #size of context for truncated BPTT
learning_rate: [0.0001,0.00001]
