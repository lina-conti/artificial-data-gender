{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment to test whether the model can predict the gender of a word based on its context at inference time, even when it has seen no gendered occurrences of the word at train time.\n",
        "\n",
        "Trains 20 models and outputs the average accuracy for each category of noun. Computes the mean and median probability of correct and incorrect predictions for each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ib2v9sZzI-"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T6qiATtTZ8wT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pcfg import PCFG\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import tempfile\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import random\n",
        "from IPython.utils import io\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_A6PHTroMem"
      },
      "source": [
        "## Get vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "HEYv79yveDco"
      },
      "outputs": [],
      "source": [
        "with open('full_vocabulary.json', 'r') as openfile:\n",
        "    full_vocab = json.load(openfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8htNOy5SpDqE"
      },
      "source": [
        "## Create train model PCFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "OtS_fDL9owT2"
      },
      "outputs": [],
      "source": [
        "# Non-terminal productions\n",
        "non_terminal_rules = \"\"\"\n",
        "S -> NP VP \".\"[1.0]\n",
        "\n",
        "PP -> PREP NP [1.0]\n",
        "VP -> VERB [0.5] | VERB NP [0.5]\n",
        "\n",
        "NP -> NPGend [0.4] | NPAmb [0.4] | NP PP [0.20]\n",
        "\n",
        "NPGend -> DETFem npGendFem [0.5] | DETMasc npGendMasc [0.5]\n",
        "npGendFem -> NOUNFemGend [0.4] | ADJFem NOUNFemGend [0.3] | NOUNFemGend ADJFem [0.3] \n",
        "npGendMasc -> NOUNMascGend [0.4] | ADJMasc NOUNMascGend [0.3] | NOUNMascGend ADJMasc [0.3] \n",
        "\n",
        "NPAmb -> DETEpic npAmbFem [0.5] | DETEpic npAmbMasc [0.5]\n",
        "npAmbFem -> NOUNFemAmb [0.4] | ADJEpic NOUNFemAmb [0.3] | NOUNFemAmb ADJEpic [0.3] \n",
        "npAmbMasc -> NOUNMascAmb [0.4] | ADJEpic NOUNMascAmb [0.3] | NOUNMascAmb ADJEpic [0.3]\n",
        "\n",
        "NOUNFemAmb -> NOUNFemAG [0.5] | NOUNFemAU [0.5]\n",
        "NOUNMascAmb -> NOUNMascAG [0.5] | NOUNMascAU [0.5]\n",
        "NOUNFemGend -> NOUNFemGG [0.5] | NOUNFemGU [0.5]\n",
        "NOUNMascGend -> NOUNMascGG [0.5] | NOUNMascGU [0.5]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# A stands for ambiguous context\n",
        "# G for gendered context\n",
        "# U for unseen (these words will not be seen when training the probe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGAiqH4fod_O",
        "outputId": "cb4ff78c-7c9c-4553-98cd-7adcf24c6b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VERB 20\n",
            "ADJMasc 100\n",
            "NOUNMascAG 50\n",
            "NOUNMascAU 50\n",
            "NOUNMascGG 50\n",
            "NOUNMascGU 50\n",
            "ADJFem 100\n",
            "NOUNFemAG 50\n",
            "NOUNFemAU 50\n",
            "NOUNFemGG 50\n",
            "NOUNFemGU 50\n",
            "ADJEpic 100\n",
            "PREP 5\n",
            "DETEpic 5\n",
            "DETMasc 5\n",
            "DETFem 5\n"
          ]
        }
      ],
      "source": [
        "# Choose subset of full vocabulary to be used\n",
        "\n",
        "nwords = {'VERB':20, 'ADJMasc':100, 'NOUNMasc':200, 'ADJFem':100, 'NOUNFem':200, 'ADJEpic':100, \n",
        "          'PREP':5, 'DETEpic':5, 'DETMasc':5, 'DETFem':5}\n",
        "        \n",
        "train_vocab = {}\n",
        "\n",
        "# convert to list  \n",
        "for k in nwords:\n",
        "    if k in {'NOUNMasc', 'NOUNFem'}:\n",
        "        train_vocab[k + 'AG'] = full_vocab[k][:nwords[k]//4]\n",
        "        train_vocab[k + 'AU'] = full_vocab[k][nwords[k]//4:nwords[k]//2]\n",
        "        train_vocab[k + 'GG'] = full_vocab[k][nwords[k]//2:nwords[k]*3//4]\n",
        "        train_vocab[k + 'GU'] = full_vocab[k][nwords[k]*3//4:nwords[k]]\n",
        "    else:\n",
        "        train_vocab[k] = full_vocab[k][:nwords[k]]\n",
        " \n",
        "for k in train_vocab:\n",
        "    print(k, len(train_vocab[k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "ToN2-h7xqcEs"
      },
      "outputs": [],
      "source": [
        "# Compute probability of each word\n",
        "\n",
        "train_probs = defaultdict(lambda: dict()) # dictionary that maps non terminal symbols to dictionaries that map words to probabilities\n",
        "\n",
        "for non_term in train_vocab:\n",
        "    for k,w in enumerate(train_vocab[non_term]):\n",
        "        N = len(train_vocab[non_term])\n",
        "        k = k+1\n",
        "        if k != N: \n",
        "            # following a zipfian distribution\n",
        "            denominateur = sum(1/n for n in range(1,N+1))\n",
        "            train_probs[non_term][w] = (1/k)/denominateur\n",
        "        else: # last word\n",
        "            train_probs[non_term][w] = 1 - sum(train_probs[non_term].values())\n",
        "    assert sum(train_probs[non_term].values()) == 1.0 # probabilites of all terminals must add to 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "sirJ88RjpaWY"
      },
      "outputs": [],
      "source": [
        "# Terminal productions\n",
        "terminal_rules = \"\\n\".join(f\"{non_term} -> \" + \" | \".join(f'\"{w}\" [{train_probs[non_term][w]}]' for w in train_probs[non_term]) for non_term in train_probs)\n",
        "train_model_rules = non_terminal_rules + terminal_rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlYarSisod_U"
      },
      "source": [
        "## Create train probe PCFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "_bIMppkvod_V"
      },
      "outputs": [],
      "source": [
        "# Non-terminal productions\n",
        "train_probe_rules = \"\"\"\n",
        "S -> NP VP \".\"[1.0]\n",
        "\n",
        "PP -> PREP NP [1.0]\n",
        "VP -> VERB [0.5] | VERB NP [0.5]\n",
        "\n",
        "NP -> NPGend [0.8] | NP PP [0.20]\n",
        "\n",
        "NPGend -> DETFem npGendFem [0.5] | DETMasc npGendMasc [0.5]\n",
        "npGendFem -> NOUNFemGend [0.4] | ADJFem NOUNFemGend [0.3] | NOUNFemGend ADJFem [0.3] \n",
        "npGendMasc -> NOUNMascGend [0.4] | ADJMasc NOUNMascGend [0.3] | NOUNMascGend ADJMasc [0.3] \n",
        "\n",
        "NOUNFemGend -> NOUNFemGG [0.5] | NOUNFemAG [0.5]\n",
        "NOUNMascGend -> NOUNMascGG [0.5] | NOUNMascAG [0.5]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Only gendered contexts and nouns marked as U (unseen) are not used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "-yR0Nraeod_X"
      },
      "outputs": [],
      "source": [
        "train_probe_rules += terminal_rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZpzY8tuod_X"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "AtSoe6SXod_Y"
      },
      "outputs": [],
      "source": [
        "model_config = \"\"\"\n",
        "context_model : 'GPT' #one of 'RNN', 'LSTM', 'GPT'\n",
        "model_input_size:  [256]\n",
        "model_output_size: [256]\n",
        "num_layers: [3]\n",
        "max_vocab_size: 50000 #Gulordava setup\n",
        "nheads : [4]  #for GPT only\n",
        "ffn_hidden : [1024]   #for GPT only\n",
        "tie_weights : True\n",
        "dropout: [0.3]\n",
        "epochs: [100]\n",
        "batch_size: [64] \n",
        "bptt_chunk : [512]     #size of context for truncated BPTT\n",
        "learning_rate: [0.0005]\n",
        "warmup_epochs: [1]       #number of epochs for warmup\n",
        "warmup_batch_size : [8]  #size of batches during warmup\n",
        "restart_cycles: [4]      #number or warmup restarts for GPT only\n",
        "positional : True        #use positional embeddings or ignore them\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "NPWIwsuKod_Z"
      },
      "outputs": [],
      "source": [
        "pos2label = lambda pos: 'Fem' if 'Fem' in pos else 'Masc'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "8EuSOeOYod_Z"
      },
      "outputs": [],
      "source": [
        "w2pos = {w: pos for pos in train_vocab for w in train_vocab[pos]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "J4LubL_9od_a"
      },
      "outputs": [],
      "source": [
        "def generate_model_data(directory_name, pcfg_rules, word_probabilities, pos2label, w2pos):\n",
        "    train_grammar = PCFG.fromstring(pcfg_rules)\n",
        "\n",
        "    train_data = '\\n'.join(sent for sent in train_grammar.generate(10_000))\n",
        "    with open(directory_name + \"/train_model_data.txt\", \"w\") as outfile:\n",
        "        outfile.write(train_data)\n",
        "\n",
        "    dev_data = '\\n'.join(sent for sent in train_grammar.generate(1_000))\n",
        "    with open(directory_name + \"/dev_model_data.txt\", \"w\") as outfile:\n",
        "        outfile.write(dev_data)\n",
        "    \n",
        "    # Check for OOVs\n",
        "    train_words = {word for word in train_data.split()}\n",
        "    if not all(w in train_words for k in word_probabilities for w in word_probabilities[k]):\n",
        "        return generate_model_data(directory_name, pcfg_rules, word_probabilities, pos2label, w2pos)\n",
        "\n",
        "    # Return gender distribution\n",
        "    train_nouns = [pos2label(w2pos[w]) for w in train_data.split() if 'NOUN' in w2pos.get(w, '')]\n",
        "    df = pd.Series(train_nouns).value_counts()\n",
        "    return {'fem_in_model_data': df['Fem'], 'masc_in_model_data': df['Masc']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "Z602-f7Xod_b"
      },
      "outputs": [],
      "source": [
        "def train_model(directory_name, model_config):\n",
        "    # returns perplexity on dev set\n",
        "    !mkdir {directory_name}/model\n",
        "    with open(f\"{directory_name}/model/model.yaml\", \"w\") as f:\n",
        "        f.write(model_config)\n",
        "    cmd = f\"python Lm4Ling/nnlm.py {directory_name}/model --train_file {directory_name}/train_model_data.txt --valid_file {directory_name}/dev_model_data.txt --device_name {device}\"\n",
        "    cmd_output = os.popen(cmd).read()\n",
        "    return {'perplexity': float(re.findall(\"Perplexity (.*)[\\r|\\n]\", cmd_output)[0])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "tg0KA7kDod_c"
      },
      "outputs": [],
      "source": [
        "def generate_probe_data(directory_name, pcfg_rules):\n",
        "    train_probe_grammar = PCFG.fromstring(pcfg_rules)\n",
        "    with open(directory_name + \"/train_probe_data.txt\", \"w\") as outfile:\n",
        "        outfile.write('\\n'.join(sent for sent in train_probe_grammar.generate(1_000)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "6OpF4MLtod_c"
      },
      "outputs": [],
      "source": [
        "def train_probe(directory_name):\n",
        "    # returns gender distribution in the probe training data and the probe\n",
        "    !python Lm4Ling/nnlm.py {directory_name}/model --probe_ex_file {directory_name}/train_probe_data.txt --out_pickle {directory_name}/probe_train_ex.pk --device_name {device}\n",
        "    \n",
        "    df_train = pd.read_pickle(f\"{directory_name}/probe_train_ex.pk\")\n",
        "    df_train['pos'] = df_train['tokens'].apply(lambda w: w2pos.get(w,'NotFound'))\n",
        "    df_train = df_train[df_train.pos.str.startswith('NOUN')]\n",
        "    X_train = df_train[\"vectors\"].tolist()  \n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "\n",
        "    # Prepare labels    \n",
        "    y_train = df_train['pos'].apply(pos2label).tolist()\n",
        "\n",
        "    # Train classifier\n",
        "    classifier = LogisticRegression(penalty = 'l2', solver = 'saga', C=10)\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    dist = pd.Series(y_train).value_counts()\n",
        "    return classifier, scaler, {'fem_in_probe_data': dist['Fem'], 'masc_in_probe_data': dist['Masc']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "oZwJC48dod_d"
      },
      "outputs": [],
      "source": [
        "def generate_rules(train_model, train_probe, test_probe, terminal_rules):\n",
        "    \"\"\"\n",
        "    Produces the appropriate set of rules according to whether the words appeared\n",
        "    in a gendered or ambiguous context when training the model, whether they appeared or not\n",
        "    when training the probe and whether they should appear in a gendered or ambiguous context at train time\n",
        "    \"\"\"\n",
        "    rules = f\"\"\"\n",
        "    S -> NP VP \".\"[1.0]\n",
        "\n",
        "    PP -> PREP NP [1.0]\n",
        "    VP -> VERB [0.5] | VERB NP [0.5]\n",
        "\n",
        "    NOUNFem -> NOUNFem{train_model[0].upper()}{train_probe[0].upper()} [1.0]\n",
        "    NOUNMasc -> NOUNMasc{train_model[0].upper()}{train_probe[0].upper()} [1.0]\n",
        "    \"\"\"\n",
        "    if test_probe == 'gendered context':\n",
        "        return rules + \"\"\"\n",
        "            NP -> NPGend [0.8] | NP PP [0.20]\n",
        "\n",
        "            NPGend -> DETFem npGendFem [0.5] | DETMasc npGendMasc [0.5]\n",
        "            npGendFem -> NOUNFem [0.4] | ADJFem NOUNFem [0.3] | NOUNFem ADJFem [0.3] \n",
        "            npGendMasc -> NOUNMasc [0.4] | ADJMasc NOUNMasc [0.3] | NOUNMasc ADJMasc [0.3] \n",
        "\n",
        "\n",
        "            \"\"\" + terminal_rules\n",
        "    if test_probe == 'ambiguous context':\n",
        "        return rules + \"\"\"\n",
        "            NP -> NPAmb [0.8] | NP PP [0.20]\n",
        "\n",
        "            NPAmb -> DETEpic npAmbFem [0.5] | DETEpic npAmbMasc [0.5]\n",
        "            npAmbFem -> NOUNFem [0.4] | ADJEpic NOUNFem [0.3] | NOUNFem ADJEpic [0.3] \n",
        "            npAmbMasc -> NOUNMasc [0.4] | ADJEpic NOUNMasc [0.3] | NOUNMasc ADJEpic [0.3]\n",
        "\n",
        "\n",
        "            \"\"\" + terminal_rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "jfqLXnWKod_d"
      },
      "outputs": [],
      "source": [
        "def test_probe(directory_name, terminal_rules, w2pos, scaler, classifier, w2prob, error_data):\n",
        "    res = {}\n",
        "    for train_model in ['gendered context', 'ambiguous context']:\n",
        "            for train_probe in ['gendered context', 'unseen']:\n",
        "                for test_probe in ['gendered context', 'ambiguous context']:\n",
        "                    test_rules = generate_rules(train_model, train_probe, test_probe, terminal_rules).replace('    ', '')             \n",
        "                    test_grammar = PCFG.fromstring(test_rules)\n",
        "                    \n",
        "                    with open(directory_name + \"/test_data.txt\", \"w\") as outfile:\n",
        "                        outfile.write('\\n'.join(sent for sent in test_grammar.generate(1_000)))\n",
        "                    \n",
        "                    with io.capture_output() as captured:\n",
        "                        !python Lm4Ling/nnlm.py {directory_name}/model --probe_ex_file {directory_name + \"/test_data.txt\"} --out_pickle {directory_name + \"/test_ex.pk\"} --device_name {device}\n",
        "                                    \n",
        "                    df_test = pd.read_pickle(directory_name + \"/test_ex.pk\")\n",
        "                    df_test['pos'] = df_test['tokens'].apply(lambda w: w2pos.get(w, 'NotFound'))\n",
        "                    df_test = df_test[df_test.pos.str.startswith('NOUN')]\n",
        "                    X_test = df_test[\"vectors\"].tolist()  \n",
        "                    X_test = scaler.transform(X_test)\n",
        "                    y_test = df_test['pos'].apply(pos2label).tolist()\n",
        "\n",
        "                    code = train_model[0].upper() + train_probe[0].upper() + test_probe[0].upper()\n",
        "                    res[\"accuracy_\" + code] = classifier.score(X_test, y_test)\n",
        "                    \n",
        "                    y_hat_test = classifier.predict(X_test)\n",
        "                    error_data['correct'] += list(y_test == y_hat_test)\n",
        "                    error_data['code'] += [code] * len(y_test)\n",
        "                    error_data['probability'] += df_test['tokens'].replace(w2prob).to_list()\n",
        "\n",
        "                    if code == \"AUA\":\n",
        "                        y_hat_test = classifier.predict(X_test)\n",
        "                        confusion_list = confusion_matrix(y_test, y_hat_test, labels=['Masc','Fem']).flatten()\n",
        "                        res['true_masc_AUA'], res['false_fem_AUA'], res['false_masc_AUA'], res['true_fem_AUA'] = confusion_list\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0g5_dt3od_e"
      },
      "source": [
        "# Main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I40i58YNod_e"
      },
      "outputs": [],
      "source": [
        "data_df = []\n",
        "error_data = {'code': [], 'correct': [], 'probability': []}\n",
        "# remove the pos level of train_probs\n",
        "w2prob = dict(element for subdict in train_probs.values() for element in subdict.items())\n",
        "\n",
        "# do everything in a temp file\n",
        "for i in range(20):\n",
        "    print(f\"-------------------------------- ITERATION {i} --------------------------------\")\n",
        "    with tempfile.TemporaryDirectory() as directory_name:\n",
        "        #temp_dir = str(pathlib.Path(directory_name))\n",
        "        row = {}\n",
        "        # generate and save train and dev sets and get gender distribution\n",
        "        row.update(generate_model_data(directory_name, train_model_rules, train_probs, pos2label, w2pos))\n",
        "        # train model and get perplexity\n",
        "        row.update(train_model(directory_name, model_config))\n",
        "        # generate probe training data\n",
        "        generate_probe_data(directory_name, train_probe_rules)\n",
        "        # train probe and get gender distribution in data\n",
        "        classifier, scaler, probe_dist = train_probe(directory_name)\n",
        "        row.update(probe_dist)\n",
        "        # test the probe for all configurations and get accuracies and confusion matrix for AUA\n",
        "        row.update(test_probe(directory_name, terminal_rules, w2pos, scaler, classifier, w2prob, error_data))\n",
        "        data_df.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "1b6J2NYBod_f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>perplexity</th>\n",
              "      <th>fem_in_model_data</th>\n",
              "      <th>masc_in_model_data</th>\n",
              "      <th>fem_in_probe_data</th>\n",
              "      <th>masc_in_probe_data</th>\n",
              "      <th>accuracy_GGG</th>\n",
              "      <th>accuracy_GGA</th>\n",
              "      <th>accuracy_GUG</th>\n",
              "      <th>accuracy_GUA</th>\n",
              "      <th>accuracy_AGG</th>\n",
              "      <th>accuracy_AGA</th>\n",
              "      <th>accuracy_AUG</th>\n",
              "      <th>accuracy_AUA</th>\n",
              "      <th>true_fem_AUA</th>\n",
              "      <th>false_fem_AUA</th>\n",
              "      <th>false_masc_AUA</th>\n",
              "      <th>true_masc_AUA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19.76374</td>\n",
              "      <td>10005</td>\n",
              "      <td>10078</td>\n",
              "      <td>1052</td>\n",
              "      <td>965</td>\n",
              "      <td>0.999503</td>\n",
              "      <td>0.995612</td>\n",
              "      <td>0.999512</td>\n",
              "      <td>0.998539</td>\n",
              "      <td>0.969506</td>\n",
              "      <td>0.496117</td>\n",
              "      <td>0.961655</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>960</td>\n",
              "      <td>1027</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.91773</td>\n",
              "      <td>10125</td>\n",
              "      <td>9987</td>\n",
              "      <td>1016</td>\n",
              "      <td>968</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998017</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999003</td>\n",
              "      <td>0.968312</td>\n",
              "      <td>0.543370</td>\n",
              "      <td>0.960804</td>\n",
              "      <td>0.483375</td>\n",
              "      <td>880</td>\n",
              "      <td>899</td>\n",
              "      <td>142</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.33353</td>\n",
              "      <td>10163</td>\n",
              "      <td>9871</td>\n",
              "      <td>991</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997989</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995986</td>\n",
              "      <td>0.978196</td>\n",
              "      <td>0.611471</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.535783</td>\n",
              "      <td>779</td>\n",
              "      <td>667</td>\n",
              "      <td>293</td>\n",
              "      <td>329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19.96347</td>\n",
              "      <td>10137</td>\n",
              "      <td>9894</td>\n",
              "      <td>975</td>\n",
              "      <td>1042</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.999492</td>\n",
              "      <td>0.988884</td>\n",
              "      <td>0.973165</td>\n",
              "      <td>0.539152</td>\n",
              "      <td>0.961360</td>\n",
              "      <td>0.497003</td>\n",
              "      <td>920</td>\n",
              "      <td>939</td>\n",
              "      <td>68</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.37293</td>\n",
              "      <td>9890</td>\n",
              "      <td>9901</td>\n",
              "      <td>992</td>\n",
              "      <td>982</td>\n",
              "      <td>0.998001</td>\n",
              "      <td>0.991555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991251</td>\n",
              "      <td>0.958967</td>\n",
              "      <td>0.543521</td>\n",
              "      <td>0.948293</td>\n",
              "      <td>0.508825</td>\n",
              "      <td>477</td>\n",
              "      <td>467</td>\n",
              "      <td>507</td>\n",
              "      <td>532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>19.91596</td>\n",
              "      <td>9990</td>\n",
              "      <td>9793</td>\n",
              "      <td>968</td>\n",
              "      <td>997</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999495</td>\n",
              "      <td>0.999011</td>\n",
              "      <td>0.995686</td>\n",
              "      <td>0.961751</td>\n",
              "      <td>0.604249</td>\n",
              "      <td>0.953049</td>\n",
              "      <td>0.521825</td>\n",
              "      <td>676</td>\n",
              "      <td>708</td>\n",
              "      <td>256</td>\n",
              "      <td>376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>19.19140</td>\n",
              "      <td>9883</td>\n",
              "      <td>10054</td>\n",
              "      <td>1026</td>\n",
              "      <td>998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997514</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999516</td>\n",
              "      <td>0.969393</td>\n",
              "      <td>0.492725</td>\n",
              "      <td>0.961078</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>34</td>\n",
              "      <td>23</td>\n",
              "      <td>926</td>\n",
              "      <td>961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18.93332</td>\n",
              "      <td>10147</td>\n",
              "      <td>10043</td>\n",
              "      <td>1026</td>\n",
              "      <td>1047</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975064</td>\n",
              "      <td>0.507463</td>\n",
              "      <td>0.967419</td>\n",
              "      <td>0.491298</td>\n",
              "      <td>53</td>\n",
              "      <td>36</td>\n",
              "      <td>987</td>\n",
              "      <td>935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>19.48486</td>\n",
              "      <td>9972</td>\n",
              "      <td>10038</td>\n",
              "      <td>976</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963946</td>\n",
              "      <td>0.494081</td>\n",
              "      <td>0.963892</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>991</td>\n",
              "      <td>987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>19.10254</td>\n",
              "      <td>9887</td>\n",
              "      <td>9986</td>\n",
              "      <td>1044</td>\n",
              "      <td>953</td>\n",
              "      <td>0.999494</td>\n",
              "      <td>0.996459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.967391</td>\n",
              "      <td>0.550860</td>\n",
              "      <td>0.954252</td>\n",
              "      <td>0.488671</td>\n",
              "      <td>579</td>\n",
              "      <td>603</td>\n",
              "      <td>390</td>\n",
              "      <td>370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>19.39843</td>\n",
              "      <td>10076</td>\n",
              "      <td>10053</td>\n",
              "      <td>999</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998501</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958727</td>\n",
              "      <td>0.558338</td>\n",
              "      <td>0.971066</td>\n",
              "      <td>0.523833</td>\n",
              "      <td>594</td>\n",
              "      <td>515</td>\n",
              "      <td>444</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>19.49674</td>\n",
              "      <td>9977</td>\n",
              "      <td>9891</td>\n",
              "      <td>1044</td>\n",
              "      <td>1038</td>\n",
              "      <td>0.999517</td>\n",
              "      <td>0.973917</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.952428</td>\n",
              "      <td>0.979581</td>\n",
              "      <td>0.560323</td>\n",
              "      <td>0.963790</td>\n",
              "      <td>0.526934</td>\n",
              "      <td>992</td>\n",
              "      <td>902</td>\n",
              "      <td>64</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>19.73527</td>\n",
              "      <td>10053</td>\n",
              "      <td>10002</td>\n",
              "      <td>975</td>\n",
              "      <td>971</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.966537</td>\n",
              "      <td>0.590724</td>\n",
              "      <td>0.954592</td>\n",
              "      <td>0.496957</td>\n",
              "      <td>217</td>\n",
              "      <td>237</td>\n",
              "      <td>755</td>\n",
              "      <td>763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>19.37068</td>\n",
              "      <td>10050</td>\n",
              "      <td>10104</td>\n",
              "      <td>994</td>\n",
              "      <td>984</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964836</td>\n",
              "      <td>0.514793</td>\n",
              "      <td>0.950666</td>\n",
              "      <td>0.515060</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "      <td>898</td>\n",
              "      <td>958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18.90945</td>\n",
              "      <td>10058</td>\n",
              "      <td>9926</td>\n",
              "      <td>977</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.999524</td>\n",
              "      <td>0.999512</td>\n",
              "      <td>0.999493</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975231</td>\n",
              "      <td>0.502262</td>\n",
              "      <td>0.939828</td>\n",
              "      <td>0.502188</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1023</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>19.62028</td>\n",
              "      <td>10073</td>\n",
              "      <td>9878</td>\n",
              "      <td>993</td>\n",
              "      <td>1017</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992032</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991457</td>\n",
              "      <td>0.972860</td>\n",
              "      <td>0.572222</td>\n",
              "      <td>0.957645</td>\n",
              "      <td>0.496742</td>\n",
              "      <td>728</td>\n",
              "      <td>780</td>\n",
              "      <td>224</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.81089</td>\n",
              "      <td>10051</td>\n",
              "      <td>10027</td>\n",
              "      <td>1013</td>\n",
              "      <td>1002</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.953685</td>\n",
              "      <td>0.969871</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.963109</td>\n",
              "      <td>0.483325</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1037</td>\n",
              "      <td>970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>19.52348</td>\n",
              "      <td>10032</td>\n",
              "      <td>10047</td>\n",
              "      <td>976</td>\n",
              "      <td>1057</td>\n",
              "      <td>0.999500</td>\n",
              "      <td>0.998502</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999525</td>\n",
              "      <td>0.973779</td>\n",
              "      <td>0.559638</td>\n",
              "      <td>0.944995</td>\n",
              "      <td>0.501511</td>\n",
              "      <td>854</td>\n",
              "      <td>851</td>\n",
              "      <td>139</td>\n",
              "      <td>142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.60413</td>\n",
              "      <td>9992</td>\n",
              "      <td>9965</td>\n",
              "      <td>1012</td>\n",
              "      <td>991</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990380</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993955</td>\n",
              "      <td>0.968473</td>\n",
              "      <td>0.510174</td>\n",
              "      <td>0.974950</td>\n",
              "      <td>0.509434</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>957</td>\n",
              "      <td>994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19.79491</td>\n",
              "      <td>9913</td>\n",
              "      <td>9958</td>\n",
              "      <td>950</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.988473</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993337</td>\n",
              "      <td>0.976325</td>\n",
              "      <td>0.507929</td>\n",
              "      <td>0.959837</td>\n",
              "      <td>0.493621</td>\n",
              "      <td>971</td>\n",
              "      <td>976</td>\n",
              "      <td>56</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    perplexity  fem_in_model_data  masc_in_model_data  fem_in_probe_data   \n",
              "0     19.76374              10005               10078               1052  \\\n",
              "1     18.91773              10125                9987               1016   \n",
              "2     19.33353              10163                9871                991   \n",
              "3     19.96347              10137                9894                975   \n",
              "4     19.37293               9890                9901                992   \n",
              "5     19.91596               9990                9793                968   \n",
              "6     19.19140               9883               10054               1026   \n",
              "7     18.93332              10147               10043               1026   \n",
              "8     19.48486               9972               10038                976   \n",
              "9     19.10254               9887                9986               1044   \n",
              "10    19.39843              10076               10053                999   \n",
              "11    19.49674               9977                9891               1044   \n",
              "12    19.73527              10053               10002                975   \n",
              "13    19.37068              10050               10104                994   \n",
              "14    18.90945              10058                9926                977   \n",
              "15    19.62028              10073                9878                993   \n",
              "16    18.81089              10051               10027               1013   \n",
              "17    19.52348              10032               10047                976   \n",
              "18    19.60413               9992                9965               1012   \n",
              "19    19.79491               9913                9958                950   \n",
              "\n",
              "    masc_in_probe_data  accuracy_GGG  accuracy_GGA  accuracy_GUG   \n",
              "0                  965      0.999503      0.995612      0.999512  \\\n",
              "1                  968      1.000000      0.998017      1.000000   \n",
              "2                  959      1.000000      0.997989      1.000000   \n",
              "3                 1042      1.000000      0.979167      0.999492   \n",
              "4                  982      0.998001      0.991555      1.000000   \n",
              "5                  997      1.000000      0.999495      0.999011   \n",
              "6                  998      1.000000      0.997514      1.000000   \n",
              "7                 1047      1.000000      1.000000      1.000000   \n",
              "8                  959      1.000000      1.000000      1.000000   \n",
              "9                  953      0.999494      0.996459      1.000000   \n",
              "10                 959      1.000000      0.998501      1.000000   \n",
              "11                1038      0.999517      0.973917      1.000000   \n",
              "12                 971      1.000000      1.000000      1.000000   \n",
              "13                 984      1.000000      0.998557      1.000000   \n",
              "14                1024      0.999524      0.999512      0.999493   \n",
              "15                1017      1.000000      0.992032      1.000000   \n",
              "16                1002      1.000000      0.960346      1.000000   \n",
              "17                1057      0.999500      0.998502      1.000000   \n",
              "18                 991      1.000000      0.990380      1.000000   \n",
              "19                1012      1.000000      0.988473      1.000000   \n",
              "\n",
              "    accuracy_GUA  accuracy_AGG  accuracy_AGA  accuracy_AUG  accuracy_AUA   \n",
              "0       0.998539      0.969506      0.496117      0.961655      0.483660  \\\n",
              "1       0.999003      0.968312      0.543370      0.960804      0.483375   \n",
              "2       0.995986      0.978196      0.611471      0.956522      0.535783   \n",
              "3       0.988884      0.973165      0.539152      0.961360      0.497003   \n",
              "4       0.991251      0.958967      0.543521      0.948293      0.508825   \n",
              "5       0.995686      0.961751      0.604249      0.953049      0.521825   \n",
              "6       0.999516      0.969393      0.492725      0.961078      0.511831   \n",
              "7       1.000000      0.975064      0.507463      0.967419      0.491298   \n",
              "8       1.000000      0.963946      0.494081      0.963892      0.501006   \n",
              "9       1.000000      0.967391      0.550860      0.954252      0.488671   \n",
              "10      1.000000      0.958727      0.558338      0.971066      0.523833   \n",
              "11      0.952428      0.979581      0.560323      0.963790      0.526934   \n",
              "12      0.998997      0.966537      0.590724      0.954592      0.496957   \n",
              "13      1.000000      0.964836      0.514793      0.950666      0.515060   \n",
              "14      1.000000      0.975231      0.502262      0.939828      0.502188   \n",
              "15      0.991457      0.972860      0.572222      0.957645      0.496742   \n",
              "16      0.953685      0.969871      0.500000      0.963109      0.483325   \n",
              "17      0.999525      0.973779      0.559638      0.944995      0.501511   \n",
              "18      0.993955      0.968473      0.510174      0.974950      0.509434   \n",
              "19      0.993337      0.976325      0.507929      0.959837      0.493621   \n",
              "\n",
              "    true_fem_AUA  false_fem_AUA  false_masc_AUA  true_masc_AUA  \n",
              "0            960           1027               0              2  \n",
              "1            880            899             142             94  \n",
              "2            779            667             293            329  \n",
              "3            920            939              68             75  \n",
              "4            477            467             507            532  \n",
              "5            676            708             256            376  \n",
              "6             34             23             926            961  \n",
              "7             53             36             987            935  \n",
              "8              9              1             991            987  \n",
              "9            579            603             390            370  \n",
              "10           594            515             444            461  \n",
              "11           992            902              64             84  \n",
              "12           217            237             755            763  \n",
              "13            68             68             898            958  \n",
              "14             1              1            1023           1032  \n",
              "15           728            780             224            263  \n",
              "16             1              1            1037            970  \n",
              "17           854            851             139            142  \n",
              "18             5              5             957            994  \n",
              "19           971            976              56             35  "
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = ['perplexity', 'fem_in_model_data', 'masc_in_model_data', 'fem_in_probe_data', 'masc_in_probe_data', \n",
        "           'accuracy_GGG', 'accuracy_GGA', 'accuracy_GUG', 'accuracy_GUA', \n",
        "           'accuracy_AGG', 'accuracy_AGA', 'accuracy_AUG', 'accuracy_AUA', \n",
        "           'true_fem_AUA', 'false_fem_AUA', 'false_masc_AUA', 'true_masc_AUA']\n",
        "df = pd.DataFrame(columns=columns, data=data_df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_df = pd.DataFrame(error_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Folder where results are saved\n",
        "\n",
        "if not os.path.exists(\"results/exp_context_or_static/error_analysis\"): \n",
        "    os.makedirs(\"results/exp_context_or_static/error_analysis\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Ay0i05ovod_f"
      },
      "outputs": [],
      "source": [
        "df.to_json(\"results/exp_context_or_static/entire_df_50_50.json\")\n",
        "error_df.to_json(\"results/exp_context_or_static/error_analysis/error_df.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl83Jhebod_g"
      },
      "source": [
        "## Analyse results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>perplexity</th>\n",
              "      <th>fem_in_model_data</th>\n",
              "      <th>masc_in_model_data</th>\n",
              "      <th>fem_in_probe_data</th>\n",
              "      <th>masc_in_probe_data</th>\n",
              "      <th>accuracy_GGG</th>\n",
              "      <th>accuracy_GGA</th>\n",
              "      <th>accuracy_GUG</th>\n",
              "      <th>accuracy_GUA</th>\n",
              "      <th>accuracy_AGG</th>\n",
              "      <th>accuracy_AGA</th>\n",
              "      <th>accuracy_AUG</th>\n",
              "      <th>accuracy_AUA</th>\n",
              "      <th>true_fem_AUA</th>\n",
              "      <th>false_fem_AUA</th>\n",
              "      <th>false_masc_AUA</th>\n",
              "      <th>true_masc_AUA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19.76374</td>\n",
              "      <td>10005</td>\n",
              "      <td>10078</td>\n",
              "      <td>1052</td>\n",
              "      <td>965</td>\n",
              "      <td>0.999503</td>\n",
              "      <td>0.995612</td>\n",
              "      <td>0.999512</td>\n",
              "      <td>0.998539</td>\n",
              "      <td>0.969506</td>\n",
              "      <td>0.496117</td>\n",
              "      <td>0.961655</td>\n",
              "      <td>0.483660</td>\n",
              "      <td>960</td>\n",
              "      <td>1027</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.91773</td>\n",
              "      <td>10125</td>\n",
              "      <td>9987</td>\n",
              "      <td>1016</td>\n",
              "      <td>968</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998017</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999003</td>\n",
              "      <td>0.968312</td>\n",
              "      <td>0.543370</td>\n",
              "      <td>0.960804</td>\n",
              "      <td>0.483375</td>\n",
              "      <td>880</td>\n",
              "      <td>899</td>\n",
              "      <td>142</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.33353</td>\n",
              "      <td>10163</td>\n",
              "      <td>9871</td>\n",
              "      <td>991</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997989</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995986</td>\n",
              "      <td>0.978196</td>\n",
              "      <td>0.611471</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.535783</td>\n",
              "      <td>779</td>\n",
              "      <td>667</td>\n",
              "      <td>293</td>\n",
              "      <td>329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19.96347</td>\n",
              "      <td>10137</td>\n",
              "      <td>9894</td>\n",
              "      <td>975</td>\n",
              "      <td>1042</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.979167</td>\n",
              "      <td>0.999492</td>\n",
              "      <td>0.988884</td>\n",
              "      <td>0.973165</td>\n",
              "      <td>0.539152</td>\n",
              "      <td>0.961360</td>\n",
              "      <td>0.497003</td>\n",
              "      <td>920</td>\n",
              "      <td>939</td>\n",
              "      <td>68</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.37293</td>\n",
              "      <td>9890</td>\n",
              "      <td>9901</td>\n",
              "      <td>992</td>\n",
              "      <td>982</td>\n",
              "      <td>0.998001</td>\n",
              "      <td>0.991555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991251</td>\n",
              "      <td>0.958967</td>\n",
              "      <td>0.543521</td>\n",
              "      <td>0.948293</td>\n",
              "      <td>0.508825</td>\n",
              "      <td>477</td>\n",
              "      <td>467</td>\n",
              "      <td>507</td>\n",
              "      <td>532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>19.91596</td>\n",
              "      <td>9990</td>\n",
              "      <td>9793</td>\n",
              "      <td>968</td>\n",
              "      <td>997</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999495</td>\n",
              "      <td>0.999011</td>\n",
              "      <td>0.995686</td>\n",
              "      <td>0.961751</td>\n",
              "      <td>0.604249</td>\n",
              "      <td>0.953049</td>\n",
              "      <td>0.521825</td>\n",
              "      <td>676</td>\n",
              "      <td>708</td>\n",
              "      <td>256</td>\n",
              "      <td>376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>19.19140</td>\n",
              "      <td>9883</td>\n",
              "      <td>10054</td>\n",
              "      <td>1026</td>\n",
              "      <td>998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997514</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999516</td>\n",
              "      <td>0.969393</td>\n",
              "      <td>0.492725</td>\n",
              "      <td>0.961078</td>\n",
              "      <td>0.511831</td>\n",
              "      <td>34</td>\n",
              "      <td>23</td>\n",
              "      <td>926</td>\n",
              "      <td>961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18.93332</td>\n",
              "      <td>10147</td>\n",
              "      <td>10043</td>\n",
              "      <td>1026</td>\n",
              "      <td>1047</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975064</td>\n",
              "      <td>0.507463</td>\n",
              "      <td>0.967419</td>\n",
              "      <td>0.491298</td>\n",
              "      <td>53</td>\n",
              "      <td>36</td>\n",
              "      <td>987</td>\n",
              "      <td>935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>19.48486</td>\n",
              "      <td>9972</td>\n",
              "      <td>10038</td>\n",
              "      <td>976</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963946</td>\n",
              "      <td>0.494081</td>\n",
              "      <td>0.963892</td>\n",
              "      <td>0.501006</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>991</td>\n",
              "      <td>987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>19.10254</td>\n",
              "      <td>9887</td>\n",
              "      <td>9986</td>\n",
              "      <td>1044</td>\n",
              "      <td>953</td>\n",
              "      <td>0.999494</td>\n",
              "      <td>0.996459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.967391</td>\n",
              "      <td>0.550860</td>\n",
              "      <td>0.954252</td>\n",
              "      <td>0.488671</td>\n",
              "      <td>579</td>\n",
              "      <td>603</td>\n",
              "      <td>390</td>\n",
              "      <td>370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>19.39843</td>\n",
              "      <td>10076</td>\n",
              "      <td>10053</td>\n",
              "      <td>999</td>\n",
              "      <td>959</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998501</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.958727</td>\n",
              "      <td>0.558338</td>\n",
              "      <td>0.971066</td>\n",
              "      <td>0.523833</td>\n",
              "      <td>594</td>\n",
              "      <td>515</td>\n",
              "      <td>444</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>19.49674</td>\n",
              "      <td>9977</td>\n",
              "      <td>9891</td>\n",
              "      <td>1044</td>\n",
              "      <td>1038</td>\n",
              "      <td>0.999517</td>\n",
              "      <td>0.973917</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.952428</td>\n",
              "      <td>0.979581</td>\n",
              "      <td>0.560323</td>\n",
              "      <td>0.963790</td>\n",
              "      <td>0.526934</td>\n",
              "      <td>992</td>\n",
              "      <td>902</td>\n",
              "      <td>64</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>19.73527</td>\n",
              "      <td>10053</td>\n",
              "      <td>10002</td>\n",
              "      <td>975</td>\n",
              "      <td>971</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.966537</td>\n",
              "      <td>0.590724</td>\n",
              "      <td>0.954592</td>\n",
              "      <td>0.496957</td>\n",
              "      <td>217</td>\n",
              "      <td>237</td>\n",
              "      <td>755</td>\n",
              "      <td>763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>19.37068</td>\n",
              "      <td>10050</td>\n",
              "      <td>10104</td>\n",
              "      <td>994</td>\n",
              "      <td>984</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964836</td>\n",
              "      <td>0.514793</td>\n",
              "      <td>0.950666</td>\n",
              "      <td>0.515060</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "      <td>898</td>\n",
              "      <td>958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18.90945</td>\n",
              "      <td>10058</td>\n",
              "      <td>9926</td>\n",
              "      <td>977</td>\n",
              "      <td>1024</td>\n",
              "      <td>0.999524</td>\n",
              "      <td>0.999512</td>\n",
              "      <td>0.999493</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975231</td>\n",
              "      <td>0.502262</td>\n",
              "      <td>0.939828</td>\n",
              "      <td>0.502188</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1023</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>19.62028</td>\n",
              "      <td>10073</td>\n",
              "      <td>9878</td>\n",
              "      <td>993</td>\n",
              "      <td>1017</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992032</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991457</td>\n",
              "      <td>0.972860</td>\n",
              "      <td>0.572222</td>\n",
              "      <td>0.957645</td>\n",
              "      <td>0.496742</td>\n",
              "      <td>728</td>\n",
              "      <td>780</td>\n",
              "      <td>224</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>18.81089</td>\n",
              "      <td>10051</td>\n",
              "      <td>10027</td>\n",
              "      <td>1013</td>\n",
              "      <td>1002</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.953685</td>\n",
              "      <td>0.969871</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.963109</td>\n",
              "      <td>0.483325</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1037</td>\n",
              "      <td>970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>19.52348</td>\n",
              "      <td>10032</td>\n",
              "      <td>10047</td>\n",
              "      <td>976</td>\n",
              "      <td>1057</td>\n",
              "      <td>0.999500</td>\n",
              "      <td>0.998502</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999525</td>\n",
              "      <td>0.973779</td>\n",
              "      <td>0.559638</td>\n",
              "      <td>0.944995</td>\n",
              "      <td>0.501511</td>\n",
              "      <td>854</td>\n",
              "      <td>851</td>\n",
              "      <td>139</td>\n",
              "      <td>142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.60413</td>\n",
              "      <td>9992</td>\n",
              "      <td>9965</td>\n",
              "      <td>1012</td>\n",
              "      <td>991</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990380</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993955</td>\n",
              "      <td>0.968473</td>\n",
              "      <td>0.510174</td>\n",
              "      <td>0.974950</td>\n",
              "      <td>0.509434</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>957</td>\n",
              "      <td>994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19.79491</td>\n",
              "      <td>9913</td>\n",
              "      <td>9958</td>\n",
              "      <td>950</td>\n",
              "      <td>1012</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.988473</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993337</td>\n",
              "      <td>0.976325</td>\n",
              "      <td>0.507929</td>\n",
              "      <td>0.959837</td>\n",
              "      <td>0.493621</td>\n",
              "      <td>971</td>\n",
              "      <td>976</td>\n",
              "      <td>56</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    perplexity  fem_in_model_data  masc_in_model_data  fem_in_probe_data   \n",
              "0     19.76374              10005               10078               1052  \\\n",
              "1     18.91773              10125                9987               1016   \n",
              "2     19.33353              10163                9871                991   \n",
              "3     19.96347              10137                9894                975   \n",
              "4     19.37293               9890                9901                992   \n",
              "5     19.91596               9990                9793                968   \n",
              "6     19.19140               9883               10054               1026   \n",
              "7     18.93332              10147               10043               1026   \n",
              "8     19.48486               9972               10038                976   \n",
              "9     19.10254               9887                9986               1044   \n",
              "10    19.39843              10076               10053                999   \n",
              "11    19.49674               9977                9891               1044   \n",
              "12    19.73527              10053               10002                975   \n",
              "13    19.37068              10050               10104                994   \n",
              "14    18.90945              10058                9926                977   \n",
              "15    19.62028              10073                9878                993   \n",
              "16    18.81089              10051               10027               1013   \n",
              "17    19.52348              10032               10047                976   \n",
              "18    19.60413               9992                9965               1012   \n",
              "19    19.79491               9913                9958                950   \n",
              "\n",
              "    masc_in_probe_data  accuracy_GGG  accuracy_GGA  accuracy_GUG   \n",
              "0                  965      0.999503      0.995612      0.999512  \\\n",
              "1                  968      1.000000      0.998017      1.000000   \n",
              "2                  959      1.000000      0.997989      1.000000   \n",
              "3                 1042      1.000000      0.979167      0.999492   \n",
              "4                  982      0.998001      0.991555      1.000000   \n",
              "5                  997      1.000000      0.999495      0.999011   \n",
              "6                  998      1.000000      0.997514      1.000000   \n",
              "7                 1047      1.000000      1.000000      1.000000   \n",
              "8                  959      1.000000      1.000000      1.000000   \n",
              "9                  953      0.999494      0.996459      1.000000   \n",
              "10                 959      1.000000      0.998501      1.000000   \n",
              "11                1038      0.999517      0.973917      1.000000   \n",
              "12                 971      1.000000      1.000000      1.000000   \n",
              "13                 984      1.000000      0.998557      1.000000   \n",
              "14                1024      0.999524      0.999512      0.999493   \n",
              "15                1017      1.000000      0.992032      1.000000   \n",
              "16                1002      1.000000      0.960346      1.000000   \n",
              "17                1057      0.999500      0.998502      1.000000   \n",
              "18                 991      1.000000      0.990380      1.000000   \n",
              "19                1012      1.000000      0.988473      1.000000   \n",
              "\n",
              "    accuracy_GUA  accuracy_AGG  accuracy_AGA  accuracy_AUG  accuracy_AUA   \n",
              "0       0.998539      0.969506      0.496117      0.961655      0.483660  \\\n",
              "1       0.999003      0.968312      0.543370      0.960804      0.483375   \n",
              "2       0.995986      0.978196      0.611471      0.956522      0.535783   \n",
              "3       0.988884      0.973165      0.539152      0.961360      0.497003   \n",
              "4       0.991251      0.958967      0.543521      0.948293      0.508825   \n",
              "5       0.995686      0.961751      0.604249      0.953049      0.521825   \n",
              "6       0.999516      0.969393      0.492725      0.961078      0.511831   \n",
              "7       1.000000      0.975064      0.507463      0.967419      0.491298   \n",
              "8       1.000000      0.963946      0.494081      0.963892      0.501006   \n",
              "9       1.000000      0.967391      0.550860      0.954252      0.488671   \n",
              "10      1.000000      0.958727      0.558338      0.971066      0.523833   \n",
              "11      0.952428      0.979581      0.560323      0.963790      0.526934   \n",
              "12      0.998997      0.966537      0.590724      0.954592      0.496957   \n",
              "13      1.000000      0.964836      0.514793      0.950666      0.515060   \n",
              "14      1.000000      0.975231      0.502262      0.939828      0.502188   \n",
              "15      0.991457      0.972860      0.572222      0.957645      0.496742   \n",
              "16      0.953685      0.969871      0.500000      0.963109      0.483325   \n",
              "17      0.999525      0.973779      0.559638      0.944995      0.501511   \n",
              "18      0.993955      0.968473      0.510174      0.974950      0.509434   \n",
              "19      0.993337      0.976325      0.507929      0.959837      0.493621   \n",
              "\n",
              "    true_fem_AUA  false_fem_AUA  false_masc_AUA  true_masc_AUA  \n",
              "0            960           1027               0              2  \n",
              "1            880            899             142             94  \n",
              "2            779            667             293            329  \n",
              "3            920            939              68             75  \n",
              "4            477            467             507            532  \n",
              "5            676            708             256            376  \n",
              "6             34             23             926            961  \n",
              "7             53             36             987            935  \n",
              "8              9              1             991            987  \n",
              "9            579            603             390            370  \n",
              "10           594            515             444            461  \n",
              "11           992            902              64             84  \n",
              "12           217            237             755            763  \n",
              "13            68             68             898            958  \n",
              "14             1              1            1023           1032  \n",
              "15           728            780             224            263  \n",
              "16             1              1            1037            970  \n",
              "17           854            851             139            142  \n",
              "18             5              5             957            994  \n",
              "19           971            976              56             35  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_json(\"results/exp_context_or_static/entire_df_50_50.json\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19.4122\n",
            "0.1518\n"
          ]
        }
      ],
      "source": [
        "print(round(df['perplexity'].mean(), 4))\n",
        "print(round(df['perplexity'].sem()*1.96, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "TFzJiYSIod_g"
      },
      "outputs": [],
      "source": [
        "accuracies_G = [\"accuracy_GGG\",\"accuracy_GGA\",\"accuracy_AGG\",\"accuracy_AGA\"]\n",
        "accuracies_U = [\"accuracy_GUG\",\"accuracy_GUA\",\"accuracy_AUG\",\"accuracy_AUA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "jFWOUp_dod_g",
        "outputId": "57b4e951-c97a-4bd8-d421-db62ad692ad7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>sem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>accuracy_GGG</th>\n",
              "      <td>0.999777</td>\n",
              "      <td>0.000105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_GGA</th>\n",
              "      <td>0.992801</td>\n",
              "      <td>0.002330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_AGG</th>\n",
              "      <td>0.969596</td>\n",
              "      <td>0.001344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_AGA</th>\n",
              "      <td>0.537971</td>\n",
              "      <td>0.008421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  mean       sem\n",
              "accuracy_GGG  0.999777  0.000105\n",
              "accuracy_GGA  0.992801  0.002330\n",
              "accuracy_AGG  0.969596  0.001344\n",
              "accuracy_AGA  0.537971  0.008421"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tab1 = pd.DataFrame({'mean': df[accuracies_G].mean(), 'sem': df[accuracies_G].sem()})\n",
        "#tab1.to_latex('results/exp1/accuracies_G.txt')\n",
        "tab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>sem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>accuracy_GUG</th>\n",
              "      <td>0.999875</td>\n",
              "      <td>0.000061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_GUA</th>\n",
              "      <td>0.992612</td>\n",
              "      <td>0.003125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_AUG</th>\n",
              "      <td>0.958440</td>\n",
              "      <td>0.001907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_AUA</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.003411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  mean       sem\n",
              "accuracy_GUG  0.999875  0.000061\n",
              "accuracy_GUA  0.992612  0.003125\n",
              "accuracy_AUG  0.958440  0.001907\n",
              "accuracy_AUA  0.503644  0.003411"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tab2 = pd.DataFrame({'mean': df[accuracies_U].mean(), 'sem': df[accuracies_U].sem()})\n",
        "#tab2.to_latex('results/exp1/accuracies_U.txt')\n",
        "tab2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word frequency impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_df = pd.read_json(\"results/exp_context_or_static/error_analysis/error_df.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>correct</th>\n",
              "      <th>probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GGG</td>\n",
              "      <td>True</td>\n",
              "      <td>0.037044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GGG</td>\n",
              "      <td>True</td>\n",
              "      <td>0.006537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GGG</td>\n",
              "      <td>True</td>\n",
              "      <td>0.044452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GGG</td>\n",
              "      <td>True</td>\n",
              "      <td>0.222261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GGG</td>\n",
              "      <td>True</td>\n",
              "      <td>0.037044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320993</th>\n",
              "      <td>AUA</td>\n",
              "      <td>False</td>\n",
              "      <td>0.022226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320994</th>\n",
              "      <td>AUA</td>\n",
              "      <td>False</td>\n",
              "      <td>0.017097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320995</th>\n",
              "      <td>AUA</td>\n",
              "      <td>True</td>\n",
              "      <td>0.222261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320996</th>\n",
              "      <td>AUA</td>\n",
              "      <td>True</td>\n",
              "      <td>0.222261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320997</th>\n",
              "      <td>AUA</td>\n",
              "      <td>False</td>\n",
              "      <td>0.007409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320998 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       code  correct  probability\n",
              "0       GGG     True     0.037044\n",
              "1       GGG     True     0.006537\n",
              "2       GGG     True     0.044452\n",
              "3       GGG     True     0.222261\n",
              "4       GGG     True     0.037044\n",
              "...     ...      ...          ...\n",
              "320993  AUA    False     0.022226\n",
              "320994  AUA    False     0.017097\n",
              "320995  AUA     True     0.222261\n",
              "320996  AUA     True     0.222261\n",
              "320997  AUA    False     0.007409\n",
              "\n",
              "[320998 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "probability\n",
              "(0.0741, 0.222]      106447\n",
              "(0.0278, 0.0741]      78229\n",
              "(0.0106, 0.0278]      71673\n",
              "(0.00345, 0.0106]     64649\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.qcut(error_df['probability'], 5, duplicates='drop').value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_labels = ['very infrequent', 'infrequent', 'frequent', 'very frequent']\n",
        "error_df['probability_interval'] = pd.qcut(error_df['probability'], 5, duplicates='drop', labels=freq_labels)\n",
        "error_df['inference_context'] = error_df['code'].apply(lambda code: 'ambiguous' if code[2] == 'A' else 'gendered')\n",
        "error_df['training_context'] = error_df['code'].apply(lambda code: 'ambiguous' if code[0] == 'A' else 'gendered')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>inference_context</th>\n",
              "      <th>ambiguous</th>\n",
              "      <th>gendered</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>probability_interval</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>very infrequent</th>\n",
              "      <td>0.7387 ±0.0048</td>\n",
              "      <td>0.984 ±0.0014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>infrequent</th>\n",
              "      <td>0.748 ±0.0045</td>\n",
              "      <td>0.9808 ±0.0014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>frequent</th>\n",
              "      <td>0.7625 ±0.0042</td>\n",
              "      <td>0.9813 ±0.0013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>very frequent</th>\n",
              "      <td>0.773 ±0.0036</td>\n",
              "      <td>0.9819 ±0.0011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "inference_context          ambiguous        gendered\n",
              "probability_interval                                \n",
              "very infrequent       0.7387 ±0.0048   0.984 ±0.0014\n",
              "infrequent             0.748 ±0.0045  0.9808 ±0.0014\n",
              "frequent              0.7625 ±0.0042  0.9813 ±0.0013\n",
              "very frequent          0.773 ±0.0036  0.9819 ±0.0011"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = error_df.groupby(['probability_interval', 'inference_context'])['correct'].mean().unstack()\n",
        "ci = error_df.groupby(['probability_interval', 'inference_context'])['correct'].sem().unstack()*1.96\n",
        "tab_probs = mean.combine(ci, lambda m, i: round(m, 4).astype('str') + ' ±' + round(i, 4).astype('str'))\n",
        "\n",
        "#tab_probs.to_latex('results/exp_context_or_static/error_analysis/frequencies_table.txt')\n",
        "tab_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='code', ylabel='probability'>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = sns.boxplot(data=error_df, y='probability', x='code', hue='correct', orient='v')\n",
        "ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='code', ylabel='probability'>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = sns.barplot(data=error_df, y='probability', x='code', hue='correct', orient='v')\n",
        "ax"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gendered_codes = [\"GGG\",\"GGA\",\"AGG\",\"AGA\"]\n",
        "unseen_codes = [\"GUG\",\"GUA\",\"AUG\",\"AUA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>inference_context</th>\n",
              "      <th>ambiguous</th>\n",
              "      <th>gendered</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training_context</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ambiguous</th>\n",
              "      <td>0.538 ±0.0049</td>\n",
              "      <td>0.9695 ±0.0017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gendered</th>\n",
              "      <td>0.9928 ±0.0008</td>\n",
              "      <td>0.9998 ±0.0001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "inference_context       ambiguous        gendered\n",
              "training_context                                 \n",
              "ambiguous           0.538 ±0.0049  0.9695 ±0.0017\n",
              "gendered           0.9928 ±0.0008  0.9998 ±0.0001"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = error_df[error_df.code.isin(gendered_codes)].groupby(['training_context','inference_context'])['correct'].mean().unstack()\n",
        "sem = error_df[error_df.code.isin(gendered_codes)].groupby(['training_context','inference_context'])['correct'].sem().unstack()*1.96\n",
        "tab1 = mean.combine(sem, lambda m, e: round(m, 4).astype('str') + ' ±' + round(e, 4).astype('str'))\n",
        "\n",
        "#tab1.to_latex('results/exp_context_or_static/error_analysis/accuracies_G.txt')\n",
        "tab1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>inference_context</th>\n",
              "      <th>ambiguous</th>\n",
              "      <th>gendered</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training_context</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ambiguous</th>\n",
              "      <td>0.5037 ±0.0049</td>\n",
              "      <td>0.9584 ±0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gendered</th>\n",
              "      <td>0.9926 ±0.0008</td>\n",
              "      <td>0.9999 ±0.0001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "inference_context       ambiguous        gendered\n",
              "training_context                                 \n",
              "ambiguous          0.5037 ±0.0049   0.9584 ±0.002\n",
              "gendered           0.9926 ±0.0008  0.9999 ±0.0001"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = error_df[error_df.code.isin(unseen_codes)].groupby(['training_context','inference_context'])['correct'].mean().unstack()\n",
        "sem = error_df[error_df.code.isin(unseen_codes)].groupby(['training_context','inference_context'])['correct'].sem().unstack()*1.96\n",
        "tab2 = mean.combine(sem, lambda m, e: round(m, 4).astype('str') + ' ±' + round(e, 4).astype('str'))\n",
        "\n",
        "#tab2.to_latex('results/exp_context_or_static/error_analysis/accuracies_U.txt')\n",
        "tab2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b961adf93988a8634fdc2b6879cad8a348a5f3df5f57a68ac1fd09f1dd31685"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
